{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_XcQ_KQwIVI"
      },
      "source": [
        "# Latent-Space Feature to Digit Classification\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "⚠️ Don't forget to save your work regularly. If you accidentally close the tab/window then all your changes might be lost. The best way to save the current state is to click on `File` then `Save a copy in GitHub`. Select `deepsafety/YOUR_NAME` as repository and feel free to overwrite `latent_to_digit.ipynb`. Also use a sensible commit message. Then click the `OK` button to save the changes.\n",
        "\n",
        "---\n",
        "\n",
        "⚠️ You don't have to answer all the questions in detail. We will discuss them later. If you thinks it might be helpful to add some keywords or hints below the questions, feel free to add as many as you like.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "We created an encoder which encoded each digit in the MNIST data set into a vector of 16 (called `FEATURE_SIZE` in the code below) floating-point values. Additionally, just like in the original MNIST data set, each feature vector represents a digit. Hence we have a mapping from feature vectors to digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7aWmCefwx-Z"
      },
      "source": [
        "Let's download the assignment library first. Feel free to check the code in `https://github.com/sven-ds/assignments` if you feel unsure aboute running it. You will not need any details hidden in there for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajs8YcSEwNDU",
        "outputId": "8e2f0920-39b0-4964-bb11-4b0e875dbfb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'assignments'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 15 (delta 2), reused 10 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (15/15), 4.78 MiB | 7.86 MiB/s, done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!rm -rf assignments\n",
        "!git clone https://github.com/sven-ds/assignments\n",
        "!python3 -m pip install matplotlib numpy torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAT1xgrQxNIw"
      },
      "source": [
        "Let's initialize the assignment..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "ly6xtBhWAqGJ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0Jb17q7KwIVW"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"assignments/assignment01\")\n",
        "\n",
        "import latentclassification\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import utils\n",
        "\n",
        "# device on which the calculations are running\n",
        "DEVICE = utils.best_device()\n",
        "\n",
        "# this data set is used for training\n",
        "DATA_SET = \"assignments/assignment01/latent_to_digit.pickle\"\n",
        "\n",
        "# the size of the feature vector\n",
        "FEATURE_SIZE = 16\n",
        "\n",
        "# number of digits; this is more or less obviously 10\n",
        "NUM_DIGITS = 10\n",
        "\n",
        "# this is the batch size used for learning; feel free to change it to any\n",
        "# sensible value\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# load the test data and the validation data\n",
        "training_data = latentclassification.load_data(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    training=True,\n",
        "    max_size=2000,\n",
        "    filename=DATA_SET)\n",
        "validation_data = latentclassification.load_data(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    training=False,\n",
        "    filename=DATA_SET)\n",
        "input_size = 16 \n",
        "hidden_size = 50 \n",
        "num_classes=10"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnp3Osjy2eq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for samples, targets in training_data:\n",
        "  print(samples)\n",
        "  break"
      ],
      "metadata": {
        "id": "eRptpzLqxXaU",
        "outputId": "f130f61d-0234-43f9-9f61-c5891496c77a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.8394, 3.1514, 3.2021,  ..., 2.0237, 2.8211, 3.5323],\n",
            "        [3.2885, 1.7961, 0.2678,  ..., 0.3805, 1.9249, 3.2270],\n",
            "        [0.1610, 6.5332, 1.5935,  ..., 3.1080, 1.4980, 2.7001],\n",
            "        ...,\n",
            "        [4.3933, 6.2330, 2.0271,  ..., 5.0460, 0.3134, 5.2163],\n",
            "        [3.6562, 4.3396, 6.6165,  ..., 3.4630, 6.3188, 3.7964],\n",
            "        [5.0571, 2.2256, 2.0859,  ..., 2.3870, 2.9667, 4.4254]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples.shape"
      ],
      "metadata": {
        "id": "8z2lE5zTxhxy",
        "outputId": "2627bdce-9ab8-4a79-9809-f0e994235348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples[1]"
      ],
      "metadata": {
        "id": "Tmx9WV0I61hz",
        "outputId": "e834a14e-8d7c-4e46-b52b-ae346468d726",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.2885, 1.7961, 0.2678, 1.0592, 2.6752, 0.3772, 0.0000, 3.9215, 1.9425,\n",
              "        0.1905, 0.3023, 2.3061, 3.1465, 0.3805, 1.9249, 3.2270])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "id": "SF0Auf7exrLu",
        "outputId": "72f928a0-4973-4c57-dfb9-9d58c408468f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([8, 1, 2, 4, 1, 6, 6, 3, 5, 2, 8, 5, 5, 6, 4, 0, 1, 5, 2, 2, 0, 7, 6, 1,\n",
              "        2, 9, 6, 9, 9, 3, 3, 5, 1, 7, 5, 5, 1, 9, 8, 0, 0, 0, 8, 9, 5, 4, 0, 9,\n",
              "        8, 1, 8, 0, 7, 4, 1, 4, 9, 8, 4, 5, 6, 1, 2, 2, 9, 3, 1, 9, 1, 3, 0, 0,\n",
              "        8, 9, 3, 8, 5, 1, 6, 8, 3, 6, 5, 3, 4, 4, 4, 4, 7, 8, 5, 3, 9, 9, 4, 5,\n",
              "        7, 2, 6, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(imgs):\n",
        "    imgs = imgs / 2 + 0.5   # unnormalize\n",
        "    npimgs = imgs.numpy()\n",
        "    plt.imshow(np.transpose(npimgs, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# one batch of random training images\n",
        "dataiter = iter(training_data)\n",
        "data = next(dataiter)\n",
        "img_grid = torchvision.utils.make_grid(images[0:25], nrow=5)\n",
        "imshow(img_grid)"
      ],
      "metadata": {
        "id": "rp9mB0-L_ze7",
        "outputId": "7a6f3b01-72c9-4b4c-edda-f47c929e33fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WC9l04swIVX"
      },
      "source": [
        "## The Model\n",
        "\n",
        "Implement the `forward` (and optinoally `__init__`) method(s) below.\n",
        "\n",
        "The input `x` of the `forward` method is a batch consisting of `BATCH_SIZE` entries with `FEATURE_SIZE` float values. E.g.\n",
        "\n",
        "```\n",
        "[[3.3629, 5.0030, 2.1696, 2.1625, 4.4470, 3.5022, 1.6455, 2.7648, 2.2284, 2.5960, 1.6632, 1.9659, 0.8189, 1.6301, 0.7227, 4.4145],\n",
        " [4.2772, 2.6310, 0.3583, 1.5026, 3.4517, 0.5529, 0.0233, 4.3412, 2.3399, 1.5852, 1.3101, 2.3642, 3.6748, 0.0227, 0.9982, 3.6081],\n",
        " ...]\n",
        " ```\n",
        " \n",
        " The output is encoded as a tensor consisting of `BATCH_SIZE` entries. Each row contains the expected digit. E.g.\n",
        " \n",
        " ```\n",
        " [[1] # representing digit 1\n",
        "  [8] # representing digit 8\n",
        "  ...]\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FPWuCnS2o7J"
      },
      "source": [
        " **Task**: Implement a classifier which expects a feature vector as input and outputs a one-hot encoded digit.\n",
        " \n",
        " 1. implement `Model`\n",
        " 2. choose a loss function\n",
        " 3. choose an optimizer\n",
        " 4. choose sensible parameters for the number of epochs and the learning rate\n",
        " 5. feel free to add more parameters to the optimizer\n",
        " 6. train the model\n",
        " \n",
        " \n",
        "  **Hint:** The training data set is very small on purpose. Therefore, training an epoch should only take fractions of a second. If a single epoch takes much longer, your network is likely too complex.\n",
        "\n",
        "A fairly simple approach should already produce good results after 200 epochs. Don't expect the accuraccy to be higher than 95%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "vtH7zMBJwIVY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "8fead240-3f02-4464-a901-d9d2a5dfe4d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-e604515cda16>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     device=DEVICE)\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m losses = trainer.fit(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/assignments/assignment01/latentclassification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_epochs, training_data, validation_data, verbose)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             training_loss = self.train_epoch(\n\u001b[0m\u001b[1;32m    130\u001b[0m                 data_loader=training_data)\n\u001b[1;32m    131\u001b[0m             validation_loss = self.validate(\n",
            "\u001b[0;32m/content/assignments/assignment01/latentclassification.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(input_size, hidden_size) \n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.l2 = torch.nn.Linear(hidden_size,10)  \n",
        "        # feel free to initialize your model\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "\n",
        "        # no activation and no softmax at the end\n",
        "        return out\n",
        "# TODO: choose an appropriate loss function\n",
        "#model = Model(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# TODO: choose an optimizer\n",
        "optimizer = optim.SGD(Model().parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# TODO: choose a sensible number of epochs for the training;\n",
        "# feel free to experiment with it;\n",
        "# you can check your assumptions with the graph below\n",
        "NUM_EPOCHS =200\n",
        "\n",
        "# TODO: choose a sensible learning rate for your optimizer\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# this is the batch size use for learning; feel free to change it to any\n",
        "# sensible value\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# are there any other parameters you want to pass to the optimizer?\n",
        "OPTIMIZER_KWARGS = dict()\n",
        "# the code below starts the training of the model\n",
        "model = Model().to(DEVICE)\n",
        "trainer = latentclassification.Trainer(\n",
        "    model=model,\n",
        "    loss_function=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE)\n",
        "\n",
        "losses = trainer.fit(\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    training_data=training_data,\n",
        "    validation_data=validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim \n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(input_size, hidden_size) \n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.l2 = torch.nn.Linear(hidden_size, num_classes)  \n",
        "        # feel free to initialize your model\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        return out\n",
        "# TODO: choose an appropriate loss function\n",
        "#model = Model(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "loss_function =  torch.nn.MSELoss()\n",
        "\n",
        "# TODO: choose an optimizer\n",
        "optimizer = optim.SGD(Model().parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# TODO: choose a sensible number of epochs for the training;\n",
        "# feel free to experiment with it;\n",
        "# you can check your assumptions with the graph below\n",
        "NUM_EPOCHS =200\n",
        "\n",
        "# TODO: choose a sensible learning rate for your optimizer\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# this is the batch size use for learning; feel free to change it to any\n",
        "# sensible value\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "from torch.autograd import Variable\n",
        "for epoch in range(20):\n",
        "    for i, (images, labels) in enumerate(training_data):\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [%d/%d], Iter [%d] Loss: %.4f' %(epoch+1, 10, i+1, loss.data[0]))"
      ],
      "metadata": {
        "id": "1GUIe5fb5uRm"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "_sE2t28e535x",
        "outputId": "708d8ab0-0906-43fc-da06-edda42563922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (l1): Linear(in_features=16, out_features=50, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[1]"
      ],
      "metadata": {
        "id": "ET0r5CTH788h",
        "outputId": "8ee312b8-2321-4c8a-83fa-cbd92a28ecc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1883, -0.8676, -0.3347,  1.0633, -0.4764,  0.7843,  0.1559, -0.3409,\n",
              "        -0.5253,  0.4625], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2oG_7S7A8QWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the code below starts the training of the model\n",
        "model = Model().to(DEVICE)\n",
        "trainer = latentclassification.Trainer(\n",
        "    model=model,\n",
        "    loss_function=loss_function(training_data,validation_data),\n",
        "    optimizer=optimizer(model.parameters(), lr=LEARNING_RATE, **OPTIMIZER_KWARGS),\n",
        "    device=DEVICE)\n",
        "\n",
        "losses = trainer.fit(\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    training_data=training_data,\n",
        "    validation_data=validation_data)"
      ],
      "metadata": {
        "id": "gdKGD1zJ4WBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpZfTamEBjtQ"
      },
      "source": [
        "Let's visualize the results in a simple plot:\n",
        "\n",
        "The left graph compares the logarith of the  training loss (red) and the logarithm of the validation loss (blue).\n",
        "\n",
        "The right graph compares the training loss  (red) and the training accuracy (green)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkKilD0gwIVZ"
      },
      "outputs": [],
      "source": [
        "utils.plot_training_vs_validation_vs_accuracy(losses, figsize=(15, 7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz-E6h1qwIVZ"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "1. How well does the model generalize?\n",
        "2. Is there any overfitting / underfitting?\n",
        "3. What is the (estimated) number of parameters/weights in your model?\n",
        "\n",
        "**Task**: Also calculate the percentage of correctly (and incorrectly) classified digits.\n",
        "\n",
        "To make things easier, a list with true and predicted values is pre-calculated. It (`matches`) contains tuples with the data `(true label, predicted label)`. For example, `(7, 5)` implies that for a feature vector labeled with 7, the label 5 has been predicted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0XnfiNgVCqr"
      },
      "outputs": [],
      "source": [
        "matches = trainer.predict_dataset(validation_data)\n",
        "\n",
        "# calcuate the number of correctly and incorrectly classified digits here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vg7t38WwIVa"
      },
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "Some classes look similar to others. For example, an 8 might be confused with a 9. Let's check if the model has these issues with the feature-vector representation.\n",
        "\n",
        "**Task**: Calculate a confusion matrix from the validation data. The row defines the true value, the column defines the predicted value.\n",
        "\n",
        "**What do you expect the confusion matrix to look like?**\n",
        "\n",
        "⚠️ You can reuse any calculations you made above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LksrKh69wIVa"
      },
      "outputs": [],
      "source": [
        "# calculate a confusion matrix here\n",
        "confusion = np.zeros((NUM_DIGITS, NUM_DIGITS))\n",
        "\n",
        "# TODO: fill the confusion matrix\n",
        "\n",
        "# this line plots the confusion matrix\n",
        "utils.plot_confusion_matrix(confusion, figsize=(12, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjahzEsgSvS1"
      },
      "source": [
        "## Precision and Recall\n",
        "\n",
        "**Task: Calculate the precion and recall values for all the digits.**\n",
        "\n",
        "Your output does not need to look fancy. It's fine if you simply plot 10 rows.\n",
        "\n",
        "⚠️ You can reuse the data in `matches` here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AWXCXfATt_7"
      },
      "outputs": [],
      "source": [
        "# calculate the precision and recall here; a simple output like\n",
        "#     0: p = 0.7; r = 0.6\n",
        "#     1: p = 0.8; r = 0.9\n",
        "#     ...\n",
        "# is sufficient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0f3jCGwwIVb"
      },
      "source": [
        "## K-Nearest-Neighbors Classifier\n",
        "\n",
        "Instead of classifying with a neural network, let's do the same task with [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) (kNN).\n",
        "\n",
        "**Task:** Implement the `classify` method of the `KNN` class. The interface description of the method is kept very vague on purpose. If it's necessary to add any pre-codnditions or clarify implementation details, please add the information to the doc string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3if4ox4rwIVb"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "def all_entries(dataset):\n",
        "    \"\"\"\n",
        "    Yields all entries of a data set.\n",
        "    \n",
        "    Each yielded entry consists of an (input, output) tuple.\n",
        "    The input is a numpy array representing the input data.\n",
        "    The output is the class label encoded as a plain integer.\n",
        "\n",
        "    E.g.\n",
        "\n",
        "        (array([3.8103154, 4.6306143, 1.7773647, 1.971603 , 3.2858315, 3.0937748,\n",
        "         2.353744 , 5.471329 , 4.2188134, 1.6467282, 2.0201886, 4.1910725,\n",
        "         5.106154 , 3.460677 , 3.6615531, 5.728415 ], dtype=float32), 3)\n",
        "    \n",
        "    for a three.\n",
        "    \"\"\"\n",
        "    for batch in dataset:\n",
        "        for x, y in zip(*batch):\n",
        "            yield x.numpy(), int(y)\n",
        "\n",
        "\n",
        "class KNN:\n",
        "    \"\"\"\n",
        "    K-nearest neighbors classifier.\n",
        "    \n",
        "    Implement the methods of this class. Feel free to add as many methods\n",
        "    or attributes as you need.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dataset):\n",
        "        \"\"\"\n",
        "        :param dataset: the training data-set\n",
        "        \"\"\"\n",
        "        # the following command collects the entire data set in a list and\n",
        "        # binds in to `self.data`. Each element in the list is a tuple of\n",
        "        # a feature vector (stored as numpy array) and its corresponding\n",
        "        # label (stored as plain integer).\n",
        "        self.data = list(all_entries(dataset))\n",
        "    \n",
        "    def classify(self, vector, k):\n",
        "        \"\"\"\n",
        "        Classify a feature vector by finding the k nearest neighbors.\n",
        "        \n",
        "        :param vector: feature vector for which the neighbors are searched\n",
        "        :param k: number of\n",
        "        :returns: the predicted label of the input vector\n",
        "        \"\"\"\n",
        "        # TODO: implement your classifier here\n",
        "        ...\n",
        "    \n",
        "\n",
        "# let's create an instance of the KNN classifier...\n",
        "knn = KNN(training_data)\n",
        "\n",
        "# ... and run it with some samples from the validation data-set.\n",
        "# The validation data is shuffled, so don't be surprised if the results vary.\n",
        "for x, y in itertools.islice(all_entries(validation_data), 10):\n",
        "    predicted = knn.classify(x, k=3)\n",
        "    \n",
        "    okay = \"X✓\"[y == predicted]\n",
        "    print(f\"{okay}; true: {y}; predicted: {predicted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp9Ic1yNwIVd"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Running the kNN with some sample values is already a good indicator.\n",
        "But what is the overall accuracy of the kNN classifier?\n",
        "\n",
        "**Task**: Write an `evaluation` method which uses your KNN implementation and calculates the accuracy for the validation data-set.\n",
        "Usa a sensible value for `k`.\n",
        "\n",
        "**Hint:** Use the `all_entries` function to iterate all the data inside the `evaluate` function.\n",
        "\n",
        "\n",
        "**Hint 2:** You can speed up the testing/debugging if you use `itertools.islice` just like above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvYNpZipwIVd"
      },
      "outputs": [],
      "source": [
        "def evaluate(classifier, data, k):\n",
        "    \"\"\"\n",
        "    Evalualte a (kNN) classifier.\n",
        "    \n",
        "    :param classifier: the classifier which is tested\n",
        "    :param data: data with which the test is run; e.g. the validation data-set\n",
        "    :param k:\n",
        "    :returns: accuracy of the classifier for the given data set in percent.\n",
        "    \"\"\"\n",
        "    # TODO: implement your evaluation here\n",
        "    ...\n",
        "\n",
        "\n",
        "K = 3\n",
        "accuracy = evaluate(knn, validation_data, k=K)\n",
        "print(f\"accuracy: {accuracy} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lu7FahXwIVd"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0JW4wJawIVd"
      },
      "source": [
        "1. What is the complexity of your implementation for the following calculations?\n",
        "    1. Initialization phase\n",
        "    1. Calculation of the distance between two feature vectors\n",
        "    1. Calculation of the k nearest neighbors\n",
        "    1. Classification of a feature vector\n",
        "2. Where could you optimize your algorithm? What would be it's complexity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrlJU2N3wIVe"
      },
      "source": [
        "## Parameter Tuning\n",
        "\n",
        "A wide range for values of `k` possible but not all of them make sense. To reach the best results and the best peformance, it's necessary to experiment with the value of `k`.\n",
        "\n",
        "**Task:** Write some code to optimize the number of neighbors take into account. Which `k` would you choose?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGI9KjjiwIVe"
      },
      "outputs": [],
      "source": [
        "# TODO: tune for k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkFaRde_Vxq5"
      },
      "source": [
        "## Model Reduction\n",
        "\n",
        "A kNN \"model\" can become very big if it contains a lot of data points. How could the model size be reduced? How would your optimization affect the results? Would it have an impact on the accuracy?\n",
        "\n",
        "⚠️ You don't have to implement your idea(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-C46ZXLWqI-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "latent_to_digit.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}